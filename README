# HOTPOT.AI - AI-Powered Cooking Game (Prototype v0.2)

**Overview:** This document outlines the development of a web-based cooking game prototype that uses a Large Language Model (LLM) as a core creative agent. The game is inspired by **Infinite Craft** and popular cooking games like *The Legend of Zelda: Breath of the Wild* and *Genshin Impact*. Players combine ingredients and cooking methods, with an LLM generating realistic culinary outcomes based on real-world cooking knowledge.

**Live Demo:**
*   **Frontend:** [Your Netlify URL Here]
*   **Backend API:** [Your Render URL Here] (Visit for a welcome message)

*(Note: The free backend service may spin down after inactivity, causing a delay on the first request.)*

## 1. Foundational Research & Conceptual Design

*(This section remains largely the same as it describes the project's inspiration and goals)*

### Comparative Analysis of Inspiration Games
Before designing our game, we examine key features of Infinite Craft and other cooking games to learn from their strengths:

| **Game**            | **Key Cooking Mechanics and Features** |
| ------------------- | -------------------------------------- |
| **Infinite Craft** (2024) | An *“endless crafting”* browser game that uses an LLM (Llama 2) to generate results for any new combination of elements. Unlike fixed-recipe games, it emphasizes open-ended discovery. Players start with basic elements and can craft anything their imagination (and the AI) allows. |
| **Breath of the Wild** (2017) | Open-world action RPG with a cooking system focused on experimentation. Players can combine up to five ingredients to discover meals. Ingredient combinations always yield a result (even if it's low-quality “Dubious Food”), rewarding creativity. |
| **Genshin Impact** (2020) | RPG with a recipe-based cooking mechanic and quality tiers (*Suspicious*, normal, or *Delicious*) based on mini-game execution. Focuses on execution rather than discovery. |
| **Overcooked** (2016) | A cooperative cooking party game emphasizing teamwork and time management under pressure. No recipe discovery. |
| **Cooking Mama** (2006) | A single-player cooking simulation focused on completing mini-games to follow fixed recipes accurately. |

**Takeaways:** Our prototype borrows the open-ended **creativity** of Infinite Craft and BOTW, letting players experiment freely. It incorporates a **quality tier** concept (Poor/Decent/Good/Excellent/Dubious) generated by the LLM, similar to Genshin's feedback system. Unlike Overcooked or Cooking Mama, the current focus is purely on **creative combination and exploration** based on real-world cooking logic.

### Core Game Loop (Prototype Implementation)
The current prototype implements a simplified version of the core loop:

1.  **Select Ingredients:** Player chooses from a list of available ingredients (can add new ones manually).
2.  **Select Cooking Method:** Player chooses from a list of available methods (can add new ones manually). Optionally adds details (e.g., "until smooth").
3.  **Combine & Cook (LLM Generation):** Player clicks "Cook!". The backend sends the combination to the Google Gemini LLM. The LLM generates a plausible outcome based on real-world cooking: a dish/item name, a modifier (e.g., "w/ garlic"), description, and quality.
4.  **Receive Dish & Feedback:** The result is displayed, indicating if it's a new discovery for the session. The generated name, modifier, description, and quality are shown.
5.  **Log & Reuse Outcome:** The discovered dish is added to a client-side "Discoveries Log". The player can optionally add the *base name* of the result back to the ingredients list for further combinations.

This loop allows for continuous experimentation and discovery within the current session.

### LLM Agent Design (Current Implementation)
The prototype uses a single LLM agent (Google Gemini Flash) acting as a "knowledgeable home chef":

-   **Role:** Generate a plausible, real-world based culinary result (name, modifier, description, quality) for any given combination of ingredients, method, and method details. It aims for accuracy over pure invention, identifying common cooking steps, intermediate products, or simple dishes.
-   **Integration:** Accessed via a REST API call from the frontend to a FastAPI backend.
-   **Caching:** The backend uses an in-memory cache to store results for specific combinations (ingredients + method + method effect) during its runtime, reducing redundant API calls and ensuring consistency within a session.
-   **Prompt Design:** A specific prompt instructs the LLM to act as a home chef, consider real recipes, and output results in a structured format (Name, Modifier, Description, Quality). Examples are provided (few-shot learning) to guide the output. (See `backend/app/llm_handler.py` for the full prompt).
-   **Quality System:** The LLM directly assigns a quality rating (Poor, Decent, Good, Excellent, Dubious) based on its assessment of the combination's culinary validity.

## 2. Current Prototype Implementation Details

This prototype demonstrates the core LLM-driven cooking mechanic.

### Tech Stack
-   **Frontend:** Plain HTML, CSS, and JavaScript. Uses a pixel-art inspired theme via CSS. Hosted on Netlify.
-   **Backend:** Python with FastAPI framework. Hosted on Render.
-   **LLM:** Google Gemini API (specifically `gemini-1.5-flash`).
-   **Caching:** Simple Python dictionary (in-memory) on the backend.
-   **Deployment:** Backend containerized with Docker.

### Implemented Features
-   **Ingredient Panel:** Displays available ingredients as buttons. Allows adding new ingredients via text input.
-   **Method Panel:** Displays available cooking methods as buttons. Allows adding new methods via text input.
-   **Method Detail Input:** Optional text field to specify how a method is applied (e.g., "for 5 minutes", "until fluffy").
-   **Cooking Station:** Allows selecting up to 5 ingredients and 1 method. Displays selected items.
-   **Cook Action:** Sends the selected combination + method detail to the backend API.
-   **Loading Indicator:** Shows feedback while waiting for the LLM response.
-   **Result Display:** Shows the LLM-generated Name, Modifier, Description, and Quality. Indicates if it's a new discovery (within the current backend cache lifetime).
-   **Add to Ingredients:** Button allows adding the *base name* of the generated result back to the ingredient list.
-   **Discovery Log:** A simple list displaying all unique dishes (Name + Modifier) discovered during the current browser session.
-   **Backend Caching:** Reduces LLM calls for repeated combinations during the server's uptime.

### Features Not Yet Implemented (from Original Plan)
-   Persistent storage (database for recipes/inventory).
-   Player accounts or saved progress.
-   Economy (currency, buying/selling, dynamic pricing).
-   Ingredient acquisition mechanics (beyond manual input).
-   Dish utilization (consuming, quests).
-   Rule-based quality modifiers (quality is purely LLM-driven).
-   Advanced AI agents (customers, market).
-   Visual pixel art assets (icons, animations).

## 3. Deployment

-   **Backend (FastAPI):** Deployed as a Docker container on **Render** (Free Tier).
    -   URL: `[Your Render URL Here]`
    -   Handles `/cook` API requests and interacts with the Gemini API.
    -   Includes in-memory caching.
    -   CORS configured to allow requests from the Netlify frontend.
-   **Frontend (HTML/CSS/JS):** Deployed as a static site on **Netlify** (Free Tier).
    -   URL: `[Your Netlify URL Here]`
    -   Contains all UI elements and client-side logic.
    -   Makes API calls to the Render backend URL.

## 4. How to Run Locally

1.  **Clone the repository:**
    ```bash
    git clone [Your Repo URL]
    cd hotpot-ai-game
    ```
2.  **Backend Setup:**
    *   Navigate to the `backend` directory: `cd backend`
    *   Create a virtual environment: `python -m venv venv`
    *   Activate the environment:
        *   Windows: `.\venv\Scripts\activate`
        *   macOS/Linux: `source venv/bin/activate`
    *   Install dependencies: `pip install -r requirements.txt`
    *   Create a `.env` file in the `backend` directory.
    *   Add your Gemini API key to the `.env` file:
        ```
        GEMINI_API_KEY=YOUR_ACTUAL_API_KEY_HERE
        ```
    *   Run the backend server: `uvicorn app.main:app --reload --port 8001`
3.  **Frontend Setup:**
    *   Navigate to the `frontend` directory: `cd ../frontend` (from `backend`) or `cd frontend` (from root).
    *   Ensure the `backendUrl` variable in `script.js` points to `http://localhost:8001`.
    *   Serve the `index.html` file. A simple way is using Python's built-in server (run from the `frontend` directory):
        ```bash
        python -m http.server 8000
        ```
    *   Open your browser and go to `http://localhost:8000`.

## 5. Next Steps (Potential Future Work)

-   Implement persistent database storage (e.g., SQLite, PostgreSQL) for discovered recipes and potentially player inventory.
-   Develop the economic loop (buying ingredients, selling dishes).
-   Introduce basic progression (unlocking methods/ingredients).
-   Refine the quality system (potentially hybrid approach).
-   Add visual pixel art assets for ingredients and UI elements.
-   Explore adding customer/quest agent interactions.